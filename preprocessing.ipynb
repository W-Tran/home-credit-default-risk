{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import gc\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['application_test.csv', 'application_train.csv', 'bureau.csv', 'bureau_balance.csv', 'credit_card_balance.csv', 'HomeCredit_columns_description.csv', 'installments_payments.csv', 'POS_CASH_balance.csv', 'previous_application.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"data/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_df = pd.read_csv(\"data/application_train.csv\")\n",
    "app_test_df = pd.read_csv(\"data/application_test.csv\")\n",
    "bureau = pd.read_csv(\"data/bureau.csv\")\n",
    "bureau_balance = pd.read_csv(\"data/bureau_balance.csv\")\n",
    "column_description_df = pd.read_csv(\"data/HomeCredit_columns_description.csv\", encoding='ISO-8859-1')\n",
    "\n",
    "train_labels = app_train_df.iloc[:,:2]\n",
    "test_id = app_test_df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "    time_before_func_call = time.time()\n",
    "    yield\n",
    "    time_after_func_call = time.time()\n",
    "    print('##################')\n",
    "    print('{} - done in {:.0f}s \\n'.format(name, time_after_func_call - time_before_func_call))\n",
    "\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "def column_description_lookup(column):\n",
    "    print(column_description_df[column_description_df['Row']==column].Description.values[0])\n",
    "\n",
    "    \n",
    "def get_app_domain_features(df):\n",
    "    df = df.copy()\n",
    "    if 'DAYS_EMPLOYED' in df:\n",
    "        df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    df['CREDIT_INCOME_PERCENT'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
    "    df['ANNUITY_INCOME_PERCENT'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['DAYS_EMPLOYED_PERCENT'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_previous_app_domain_features(df):\n",
    "    df = df.copy()\n",
    "    df['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    df['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    df['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    df['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    df['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    df['APP_CREDIT_PERC'] = df['AMT_APPLICATION'] / df['AMT_CREDIT']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_installment_domain_features(df):\n",
    "    df = df.copy()\n",
    "    df['PAYMENT_PERC'] = df['AMT_PAYMENT'] / df['AMT_INSTALMENT']\n",
    "    df['PAYMENT_DIFF'] = df['AMT_INSTALMENT'] - df['AMT_PAYMENT']\n",
    "    df['DPD'] = df['DAYS_ENTRY_PAYMENT'] - df['DAYS_INSTALMENT']\n",
    "    df['DBD'] = df['DAYS_INSTALMENT'] - df['DAYS_ENTRY_PAYMENT']\n",
    "    df['DPD'] = df['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    df['DBD'] = df['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_poly_features(df, imputer=False):\n",
    "    poly_features = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']].copy()\n",
    "    df = df.drop(columns=['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])\n",
    "    if 'TARGET' in poly_features:\n",
    "        poly_features = poly_features.drop(columns = ['TARGET'])\n",
    "    \n",
    "    if not imputer:\n",
    "        imputer = SimpleImputer(strategy = 'median')\n",
    "        poly_features = imputer.fit_transform(poly_features)\n",
    "    else:\n",
    "        poly_features = imputer.transform(poly_features)\n",
    "    \n",
    "    poly_transformer = PolynomialFeatures(degree = 3)\n",
    "    poly_transformer.fit(poly_features)\n",
    "    poly_features = poly_transformer.transform(poly_features)\n",
    "    \n",
    "    poly_features_df = pd.DataFrame(\n",
    "        poly_features, \n",
    "        columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])\n",
    "    )\n",
    "    \n",
    "    # Merge polynomial features into training dataframe\n",
    "    poly_features_df['SK_ID_CURR'] = df['SK_ID_CURR'].values\n",
    "    df = df.merge(poly_features_df, on = 'SK_ID_CURR', how = 'left')\n",
    "    \n",
    "    return df, imputer\n",
    "\n",
    "\n",
    "def agg_numeric(df, group_by_var):\n",
    "    \"\"\"\n",
    "    Aggregates the numeric values in a dataframe. This can\n",
    "    be used to create features for each instance of the grouping variable.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        df (dataframe): \n",
    "            the dataframe to calculate the statistics on\n",
    "        group_by_var (string): \n",
    "            The variable by which to group the dataframe.\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        agg (dataframe): \n",
    "            a dataframe with the statistics aggregated for all numeric columns. \n",
    "            Each instance of the grouping variable will have the statistics \n",
    "            (mean, min, max, sum; currently supported) calculated. \n",
    "            The columns are also renamed to keep track of features created.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Remove id variables other than the grouping variable\n",
    "    for col in df:\n",
    "        if col != group_by_var and 'SK_ID' in col:\n",
    "            df = df.drop(columns = col)\n",
    "    \n",
    "    numeric_df = df.select_dtypes('number').copy()\n",
    "    numeric_df[group_by_var] = df[group_by_var].values\n",
    "    agg = numeric_df.groupby(group_by_var).agg(['mean', 'max', 'min', 'sum']).reset_index()\n",
    "\n",
    "    # Flatten column names\n",
    "    agg.columns = [\n",
    "        column_name[0] + \"_\" + column_name[1] if column_name[1] else column_name[0] \\\n",
    "        for column_name in zip(agg.columns.get_level_values(0), agg.columns.get_level_values(1))\n",
    "    ]\n",
    "    \n",
    "    return agg\n",
    "\n",
    "\n",
    "def count_categorical(df, group_by_var):\n",
    "    \"\"\"\n",
    "    Computes counts and normalized counts for each observation\n",
    "    of `group_by_var` of each unique category in every categorical variable\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe \n",
    "        The dataframe to calculate the value counts for.\n",
    "        \n",
    "    group_by_var : string\n",
    "        The variable by which to group the dataframe. For each unique\n",
    "        value of this variable, the final dataframe will have one row\n",
    "    \n",
    "    Return\n",
    "    --------\n",
    "    categorical : dataframe\n",
    "        A dataframe with counts and normalized counts of each unique category in every categorical variable\n",
    "        with one row for every unique value of the `group_by_var`.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get categorical columns\n",
    "    cat_ohe = pd.get_dummies(df.select_dtypes('category')).copy()\n",
    "    cat_ohe[group_by_var] = df[group_by_var]\n",
    "    cat_agg = cat_ohe.groupby(group_by_var).agg(['sum', 'mean']).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    cat_agg.columns = [\n",
    "        column_name[0] + \"_\" + column_name[1] if column_name[1] else column_name[0] \\\n",
    "        for column_name in zip(cat_agg.columns.get_level_values(0), cat_agg.columns.get_level_values(1))\n",
    "    ]\n",
    "    \n",
    "    return cat_agg\n",
    "\n",
    "\n",
    "def deal_with_cat_feats(train_df, test_df):\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    # List Categorical Features\n",
    "    cat_feat_names = train_df.select_dtypes('category').columns.to_list()\n",
    "    train_df[cat_feat_names] = train_df[cat_feat_names].astype(\"object\").fillna(\"MISSING\")\n",
    "    test_df[cat_feat_names] = test_df[cat_feat_names].astype(\"object\").fillna(\"MISSING\")\n",
    "\n",
    "    # Remove rows for which category levels exist in train but not in test set\n",
    "    for feat in cat_feat_names:\n",
    "        cat_levels_missing_from_test = list(set(train_df[feat].value_counts().index) - set(test_df[feat].value_counts().index))\n",
    "        if cat_levels_missing_from_test:\n",
    "            train_df.drop(\n",
    "                index=train_df[train_df[feat].isin(cat_levels_missing_from_test)].index, \n",
    "                inplace=True\n",
    "            )\n",
    "\n",
    "    # label encode categorical variables\n",
    "    cat_feat_levels = {}\n",
    "\n",
    "    for feat in cat_feat_names:\n",
    "        le = LabelEncoder()\n",
    "        train_df[feat] = le.fit_transform(train_df[feat])\n",
    "        test_df[feat] = le.transform(test_df[feat])\n",
    "        cat_feat_levels[feat] = le.classes_\n",
    "\n",
    "        # Replace \"MISSING\" level with negative value (treated as missing by LGBM)\n",
    "        if \"MISSING\" in cat_feat_levels[feat]:\n",
    "            train_df[feat] = train_df[feat].replace(list(cat_feat_levels[feat]).index(\"MISSING\"), -1)\n",
    "            test_df[feat] = test_df[feat].replace(list(cat_feat_levels[feat]).index(\"MISSING\"), -1)\n",
    "            \n",
    "    train_df[cat_feat_names] = train_df[cat_feat_names].astype(\"category\")\n",
    "    test_df[cat_feat_names] = test_df[cat_feat_names].astype(\"category\")\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 286.23 MB\n",
      "Memory usage after optimization is: 59.54 MB\n",
      "Decreased by 79.2%\n",
      "Memory usage of dataframe is 45.00 MB\n",
      "Memory usage after optimization is: 9.40 MB\n",
      "Decreased by 79.1%\n",
      "Memory usage of dataframe is 222.62 MB\n",
      "Memory usage after optimization is: 78.57 MB\n",
      "Decreased by 64.7%\n",
      "Memory usage of dataframe is 624.85 MB\n",
      "Memory usage after optimization is: 156.21 MB\n",
      "Decreased by 75.0%\n",
      "##################\n",
      "Reduce memory - done in 8s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Reduce memory\"):\n",
    "    app_train_df = reduce_mem_usage(app_train_df, use_float16=True)\n",
    "    app_test_df = reduce_mem_usage(app_test_df, use_float16=True)\n",
    "    bureau = reduce_mem_usage(bureau, use_float16=True)\n",
    "    bureau_balance = reduce_mem_usage(bureau_balance, use_float16=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domain knowledge features**\n",
    "\n",
    "These domain knowledge features were inspired by the awesome community Kaggle notebooks [such as this one](https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "Get application domain knowledge features - done in 0s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Get application domain knowledge features\"):\n",
    "    app_train_df = get_app_domain_features(app_train_df)\n",
    "    app_test_df = get_app_domain_features(app_test_df)\n",
    "\n",
    "# Save intermediate dataframes\n",
    "# app_train_df.to_csv(\"app_train_with_domain_feats.csv\", index=False)\n",
    "# app_test_df.to_csv(\"app_test_wth_domain_feats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial features** \n",
    "\n",
    "These polynomial features were inspired by [this notebook](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with timer(\"Get polynomial features\"):\n",
    "#     app_train_df, missing_imputer = get_poly_features(app_train_df)\n",
    "#     app_test_df, _ = get_poly_features(app_test_df, imputer=missing_imputer)\n",
    "\n",
    "# Save intermediate dataframes\n",
    "#app_train_poly.to_csv(\"app_train_with_domain_poly_feats.csv\", index=False)\n",
    "#app_test_poly.to_csv(\"app_test_with_domain_poly_feats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Incorporate Bureau and Bureau Balance features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "Incorporate Bureau Balance features into Bureau dataframe - done in 17s \n",
      "\n",
      "##################\n",
      "Incorporate Bureau features into applications dataframe - done in 15s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Incorporate Bureau Balance features into Bureau dataframe\"):\n",
    "    bureau_balance_agg = pd.merge(\n",
    "        left = agg_numeric(bureau_balance, group_by_var='SK_ID_BUREAU'), \n",
    "        right = count_categorical(bureau_balance, group_by_var='SK_ID_BUREAU'),\n",
    "        how='inner',\n",
    "        on='SK_ID_BUREAU'\n",
    "    )\n",
    "    \n",
    "    bureau.merge(bureau_balance_agg, how='left', on='SK_ID_BUREAU')\n",
    "    \n",
    "    del bureau_balance, bureau_balance_agg\n",
    "    gc.collect()\n",
    "    \n",
    "with timer(\"Incorporate Bureau features into applications dataframe\"):\n",
    "    bureau_agg = pd.merge(\n",
    "        left = agg_numeric(bureau, group_by_var='SK_ID_CURR'), \n",
    "        right = count_categorical(bureau, group_by_var='SK_ID_CURR'),\n",
    "        how='inner',\n",
    "        on='SK_ID_CURR'\n",
    "    )\n",
    "\n",
    "    app_train_df = app_train_df.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    app_test_df = app_test_df.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    \n",
    "    del bureau, bureau_agg\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Incorporate all other features**\n",
    "\n",
    "More domain knowledge features inspired by [this](https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features) notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "Read in all other dataframes - done in 24s \n",
      "\n",
      "Memory usage of dataframe is 471.48 MB\n",
      "Memory usage after optimization is: 130.62 MB\n",
      "Decreased by 72.3%\n",
      "Memory usage of dataframe is 610.43 MB\n",
      "Memory usage after optimization is: 171.69 MB\n",
      "Decreased by 71.9%\n",
      "Memory usage of dataframe is 673.88 MB\n",
      "Memory usage after optimization is: 263.69 MB\n",
      "Decreased by 60.9%\n",
      "Memory usage of dataframe is 830.41 MB\n",
      "Memory usage after optimization is: 311.40 MB\n",
      "Decreased by 62.5%\n",
      "##################\n",
      "Reduce memory - done in 11s \n",
      "\n",
      "##################\n",
      "Get more domain knowledge features - done in 8s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Read in all other dataframes\"):\n",
    "    previous = pd.read_csv(\"data/previous_application.csv\")\n",
    "    cash = pd.read_csv(\"data/POS_CASH_balance.csv\")\n",
    "    credit = pd.read_csv('data/credit_card_balance.csv')\n",
    "    installments = pd.read_csv('data/installments_payments.csv')\n",
    "\n",
    "with timer(\"Reduce memory\"):\n",
    "    previous = reduce_mem_usage(previous, use_float16=True)\n",
    "    cash = reduce_mem_usage(cash, use_float16=True)\n",
    "    credit = reduce_mem_usage(credit, use_float16=True)\n",
    "    installments = reduce_mem_usage(installments, use_float16=True)\n",
    "    \n",
    "with timer(\"Get more domain knowledge features\"):\n",
    "    previous = get_previous_app_domain_features(previous)\n",
    "    installments = get_installment_domain_features(installments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "Incorporate previous application features into applications dataframe - done in 225s \n",
      "\n",
      "##################\n",
      "Incorporate cash loan features into applications dataframe - done in 18s \n",
      "\n",
      "##################\n",
      "Incorporate installment payments features into applications dataframe - done in 14s \n",
      "\n",
      "##################\n",
      "Incorporate credit card features into applications dataframe - done in 11s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Incorporate previous application features into applications dataframe\"):\n",
    "    previous_agg = pd.merge(\n",
    "        left = agg_numeric(previous, group_by_var='SK_ID_CURR'), \n",
    "        right = count_categorical(previous, group_by_var='SK_ID_CURR'),\n",
    "        how='inner',\n",
    "        on='SK_ID_CURR'\n",
    "    )\n",
    "    \n",
    "    # Drop columns with >90% missing values\n",
    "    previous_agg_missing = (previous_agg.isna().sum() / len(previous_agg))\n",
    "    previous_agg.drop(columns=previous_agg_missing[previous_agg_missing>=0.9].index.tolist(), inplace=True)\n",
    "    \n",
    "    app_train_df = app_train_df.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    app_test_df = app_test_df.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    \n",
    "    del previous_agg, previous_agg_missing, previous\n",
    "    gc.collect();\n",
    "    \n",
    "with timer(\"Incorporate cash loan features into applications dataframe\"):\n",
    "    cash_agg = pd.merge(\n",
    "        left = agg_numeric(cash, group_by_var='SK_ID_CURR'), \n",
    "        right = count_categorical(cash, group_by_var='SK_ID_CURR'),\n",
    "        how='inner',\n",
    "        on='SK_ID_CURR'\n",
    "    )\n",
    "    \n",
    "    app_train_df = app_train_df.merge(cash_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    app_test_df = app_test_df.merge(cash_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    \n",
    "    del cash_agg, cash\n",
    "    gc.collect();\n",
    "    \n",
    "with timer(\"Incorporate installment payments features into applications dataframe\"):\n",
    "    installments_agg = agg_numeric(installments, group_by_var='SK_ID_CURR')\n",
    "    \n",
    "    app_train_df = app_train_df.merge(installments_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    app_test_df = app_test_df.merge(installments_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    \n",
    "    del installments_agg, installments\n",
    "    gc.collect();\n",
    "    \n",
    "with timer(\"Incorporate credit card features into applications dataframe\"):\n",
    "    credit_agg = pd.merge(\n",
    "        left = agg_numeric(credit, group_by_var='SK_ID_CURR'), \n",
    "        right = count_categorical(credit, group_by_var='SK_ID_CURR'),\n",
    "        how='inner',\n",
    "        on='SK_ID_CURR'\n",
    "    )\n",
    "\n",
    "    app_train_df = app_train_df.merge(credit_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    app_test_df = app_test_df.merge(credit_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "    \n",
    "    del credit_agg, credit\n",
    "    gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1287.73 MB\n",
      "Memory usage after optimization is: 851.94 MB\n",
      "Decreased by 33.8%\n",
      "Memory usage of dataframe is 204.08 MB\n",
      "Memory usage after optimization is: 134.07 MB\n",
      "Decreased by 34.3%\n"
     ]
    }
   ],
   "source": [
    "app_train_df = reduce_mem_usage(app_train_df)\n",
    "app_test_df = reduce_mem_usage(app_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess and Label encode categorical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_df, app_test_df = deal_with_cat_feats(app_train_df, app_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NAME_CONTRACT_TYPE',\n",
       " 'CODE_GENDER',\n",
       " 'FLAG_OWN_CAR',\n",
       " 'FLAG_OWN_REALTY',\n",
       " 'NAME_TYPE_SUITE',\n",
       " 'NAME_INCOME_TYPE',\n",
       " 'NAME_EDUCATION_TYPE',\n",
       " 'NAME_FAMILY_STATUS',\n",
       " 'NAME_HOUSING_TYPE',\n",
       " 'OCCUPATION_TYPE',\n",
       " 'WEEKDAY_APPR_PROCESS_START',\n",
       " 'ORGANIZATION_TYPE',\n",
       " 'FONDKAPREMONT_MODE',\n",
       " 'HOUSETYPE_MODE',\n",
       " 'WALLSMATERIAL_MODE',\n",
       " 'EMERGENCYSTATE_MODE']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_train_df.select_dtypes(\"category\").columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_df.to_csv(\"app_train_df_preprocessed.csv\", index=False)\n",
    "app_test_df.to_csv(\"app_test_df_preprocessed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
